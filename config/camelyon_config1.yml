#opt
n_epoch: 50           # number of epochs
B: 16                 # batch size
B_seq: 1              # sequential batch size, set either to
                      # B (eager and lazy loading) or 1 (eager sequential loading)
n_epoch_warmup: 10    # number of warm-up epochs
lr: 0.0003            # learning rate
wd: 0.1               # weight decay

#dset
n_class: 1                            # number of classes
project: 'IPS-zx'
wandb: True
data_dir: 'data/camelyon/dsets'       # directory of dataset
train_fname: 'feat_train_500ep.hdf5'  # filename of extracted features of training set
test_fname: 'feat_test_500ep.hdf5'    # filename of extracted features of test set
n_worker: 64                          # number of workers
pin_memory: False                     # use pin memory in dataloader
eager: True                           # eager or lazy loading
save_dir: '/home/xx/ips/ips-main/output/'
title: 'fmow-lr0.0001'

#misc
eps: 0.000001
seed: 2021
track_efficiency: False   # for training, needs to be False
track_epoch: 0            # only relevant if efficiency stats are tracked.

#enc
is_image: False         # should a convolutional patch encoder be used?
enc_type: 'resnet50'    # used backbone, set either to 'resnet18' or 'resnet50'
pretrained: False       # should ImageNet weights be used?
n_chan_in: 1024        # number of input channels from resnet 50

#ips
shuffle: True             # should patches be shuffled?
shuffle_style: 'batch'    # 'batch' or 'instance'. 'batch' shuffles each instance of the batch the same way
n_token: 1                # number of learnable query tokens, corresponds to number of tasks
M: 100                   # memory size
M1: 100 
I: 500                   # iteration size
I1: 500                   
S: 400

#aggr
use_pos: False      # should positional encoding be used?
H: 8                # number of transformer layer heads
D: 512              # dimension of features
D_k: 64             # dimension of query/keys per head
D_v: 64             # dimension of values per head
D_inner: 2048       # intermediate layer dimension in MLP
attn_dropout: 0.1   # attention dropout
dropout: 0.1        # standard dropout

# define name, activation function of final layer and metric to be used
tasks:
  task0:
    id: 0
    name: 'metastases'
    act_fn: 'sigmoid'
    metric: 'auc'
