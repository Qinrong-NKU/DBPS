#opt
n_epoch: 150          # number of epochs
B: 16                 # batch size
B_seq: 16             # sequential batch size, set either to
                      # B (eager and lazy loading) or 1 (eager sequential loading)
n_epoch_warmup: 10    # number of warm-up epochs
lr: 0.0003            # learning rate
#lr: 0.0001            # learning rate
wd: 0.1               # weight decay

#dset
n_class: 4                      # number of classes
project: 'IPS-zx'
data_dir: '/home/xx/ips/ips-main/traffic'  # directory of dataset
n_worker: 4                     # number of workers
pin_memory: True                # use pin memory in dataloader
eager: True                     # eager or lazy loading
save_dir: '/home/xx/ips/ips-main/output/'
title: 'traffic-lr0.0003'
wandb: True

#misc
eps: 0.000001
seed: 0
track_efficiency: False   # for training, needs to be False
track_epoch: 0            # only relevant if efficiency stats are tracked.

#enc
is_image: True          # should a convolutional patch encoder be used?
enc_type: 'resnet18'    # used backbone, set either to 'resnet18' or 'resnet50'
pretrained: True        # should ImageNet weights be used?
n_chan_in: 3            # number of input channels
n_res_blocks: 4         # number of residual ResNet blocks

#ips
shuffle: True                 # should patches be shuffled?
shuffle_style: 'batch'        # shuffle each instance the same way? 'batch' or 'instance'
n_token: 1                    # Number of learnable query tokens
N: 192                        # Number of total patches, needs to be consistent with patch size/stride
N1: 192 
M: 10                         # memory size
M1: 10
I: 32                         # iteration size
I1: 32
S: 3 
patch_size: [100, 100]        # dims of patch
patch_stride: [100, 100]      # stride of patch

#aggr
use_pos: False        # should positional encoding be used?
H: 8                  # number of transformer layer heads
D: 512                # dimension of features
D_k: 64               # dimension of query/keys per head
D_v: 64               # dimension of values per head
D_inner: 2048         # hidden dimension of MLP
attn_dropout: 0.1     # attention dropout
dropout: 0.1          # standard dropout

tasks:
  task0:
    id: 0
    name: 'sign'
    act_fn: 'softmax'
    metric: 'accuracy'
